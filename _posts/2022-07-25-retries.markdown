---
title: Retries e lidando com erros transientes 101
published: true
description: Lidando com erros transientes atrav√©s de retries.
tags: sistemasdistribuidos, retries, microservi√ßos
---

Se voc√™ leu o primeiro post dessa s√©rie sobre sistemas distribu√≠dos voc√™ aprendeu que [Sistemas distribu√≠dos s√£o estranhos](https://dev.to/hugaomarques/sistemas-distribuidos-sao-estranhos-5bkp). A partir do momento que separamos os sistemas em computadores diferentes atrav√©s da rede, um mundo
novo de possibilidades e problemas se tornam acess√≠veis a n√≥s. Entre eles, nossos sistemas agora s√£o suscet√≠veis a erros transientes.

Nesse post, n√≥s vamos discutir essas falhas tempor√°rias, como podemos tentar resolver esse problema e quais novos problemas s√£o gerados como parte de nossa solu√ß√£o.

## Problema: Erros transientes

Se voc√™ trabalha com software voc√™ j√° deve ter esbarrado em um **erro persistente**, por exemplo, um bug no seu sistema que d√° erro 100% das vezes que determinada condi√ß√£o ocorre ou quando o servidor sai do ar totalmente.
Uma outra categoria de erro s√£o os erros transientes ou erros tempor√°rios. Esses erros normalmente ocorrem de forma inesperada, muitas vezes duram apenas 1 segundo ou apenas alguns milisegundos, apenas o suficiente pra estregar o sucesso da sua requisi√ß√£o ü§° .

Se lembrarmos do [post anterior](https://dev.to/hugaomarques/sistemas-distribuidos-sao-estranhos-5bkp), as fal√°cias dos sistemas distribu√≠dos ajudam a explicar os erros transientes:
1. A rede √© confi√°vel: A rede vai falhar. Ela pode falhar por apenas 1 segundo. Mas isso pode ser suficiente para impedir aquela requisi√ß√£o que seu sistema tentou fazer com sucesso.
2. A topologia n√£o muda: Em tempos de cloud, m√°quinas s√£o adicionadas e removidas da rede o tempo inteiro. Imagine o que acontece se o sistema A depende do sistema B e, no momento que a requisi√ß√£o A -> B √© feita, o sistema B est√° executando um deploy e a m√°quina respons√°vel por atender a requisi√ß√£o √© removida da rede.
3. A topologia n√£o muda: Com aparelhos mobile, usu√°rios est√£o desconectando e reconectando em redes diferentes o tempo inteiro. O que acontece quando o usu√°rio est√° executando uma requisi√ß√£o nos segundos de desconex√£o da rede?

Os exemplos s√£o infind√°veis. Em resumo, sempre haver√° a chance que a sua primeira tentativa de executar uma requisi√ß√£o vai falhar.

### Exemplo

> Disclaimer: O c√≥digo abaixo n√£o passa na qualidade pra ir pra produ√ß√£o üò¨.

Ok, muita lorota mas que tal simularmos o nosso problema? O c√≥digo completo pode ser encontrado no [github](https://github.com/hugomarques/code_examples/tree/main/retries).

![shut up and show me the code](https://raw.githubusercontent.com/hugomarques/hugomarques.github.io/main/_assets/shut-up-and-show-me-the-code.jpg)

O m√©todo abaixo roda em uma pequena App com Spring Boot. Em resumo, o m√©todo falha todo minuto entre 00 e 05 segundos.

```java
@RequestMapping("/")
public ResponseEntity<String> home() throws InterruptedException {
  final var now = LocalDateTime.now();
  logger.info("Failure flag value: " + now);
  // Sempre envie um erro durante os primeiros 5s de cada minuto.
  if (now.getSecond() >= 0 && now.getSecond() <= 5)
    throw new IllegalStateException();
  return ResponseEntity.status(200).body("Current time: " + now);
}
```


Eu escrevi um cliente que invoca essa app continuamente, por√©m, se o cliente percebe 1 erro, o cliente interrompe a execu√ß√£o.

```java
public static void main(String[] args) throws URISyntaxException, InterruptedException {

  final HttpRequest request = HttpRequest.newBuilder()
      .uri(new URI("http://localhost:8080"))
      .GET()
      .build();

  while (NUM_ERRORS < 1) {
    try {
      var response = callService(request)
      NUM_ERRORS = 0;
      System.out.println("--------------------------------------------");
      System.out.println(Thread.currentThread().getName());
      System.out.println(response.statusCode());
      System.out.println(response.body());
      System.out.println("--------------------------------------------");
    } catch (RuntimeException ex) {
      System.out.println(ex.getMessage());
      NUM_ERRORS++;
    }
    Thread.sleep(500);
  }
}

private static HttpResponse<String> callService(HttpRequest request) {
  try {
    System.out.println("Executing request at" + LocalDateTime.now());
    final HttpResponse<String> response =
        HTTP_CLIENT.send(request, HttpResponse.BodyHandlers.ofString());
    if (response.statusCode() == 500)
      throw new RuntimeException("Failed to call API");
    return response;
  } catch (IOException e) {
    System.out.println("Operation failed, exception IO");
    throw new RuntimeException(e);
  } catch (InterruptedException e) {
    System.out.println("Operation failed, exception Interrupted");
    throw new RuntimeException(e);
  }
}
```

Nossa execu√ß√£o pode gerar o seguinte log:

```log
main
200
Current time: 2022-07-26T18:19:59.238087
--------------------------------------------
Executing request at2022-07-26T18:19:59.743644
--------------------------------------------
main
200
Current time: 2022-07-26T18:19:59.744904
--------------------------------------------
Executing request at2022-07-26T18:20:00.250584
Failed to call API
```

E a√≠? Como a gente pode evitar que nossa aplica√ß√£o pare com apenas 1 erro?

## Retries lineares

A forma mais simples de contornamos um erro transiente √© tentar a mesma requisi√ß√£o de novo. Existem diversas bibliotecas que ajudam com isso como
`sprint-boot-retry` e o `resilience4j`. No nosso exemplo aqui, eu implementei com `resilience4j`.

O c√≥digo principal segue abaixo:

```java
 public static void main(String[] args) throws URISyntaxException, InterruptedException {

    final HttpRequest request = HttpRequest.newBuilder()
        .uri(new URI("http://localhost:8080"))
        .GET()
        .build();

    Function<HttpRequest, HttpResponse> service = (HttpRequest x) -> callService(x);

    var config = RetryConfig.custom().retryExceptions(Exception.class).maxAttempts(3).waitDuration(
        Duration.ofSeconds(3)).build();
    var registry = RetryRegistry.of(config);
    var retry = registry.retry("retry");

    final var retryableServiceCall = Retry.decorateFunction(retry, service);


    while (NUM_ERRORS < 1) {
      try {
        var response = retryableServiceCall.apply(request);
        NUM_ERRORS = 0;
        System.out.println("--------------------------------------------");
        System.out.println(Thread.currentThread().getName());
        System.out.println(response.statusCode());
        System.out.println(response.body());
        System.out.println("--------------------------------------------");
      } catch (RuntimeException ex) {
        System.out.println(ex.getMessage());
        NUM_ERRORS++;
      }
      Thread.sleep(500);
    }
  }
```

Um poss√≠vel resultado de executar o c√≥digo acima seria:

```
main
200
Current time: 2022-07-27T17:05:59.227065
--------------------------------------------
Executing request at2022-07-27T17:05:59.731737
--------------------------------------------
main
200
Current time: 2022-07-27T17:05:59.733363
--------------------------------------------
Executing request at2022-07-27T17:06:00.238117
Executing request at2022-07-27T17:06:03.303864
Executing request at2022-07-27T17:06:06.316072
--------------------------------------------
main
200
Current time: 2022-07-27T17:06:06.318999
```

Observe como a execu√ß√£o roda a cada 500ms mas o seguinte comportamente acontece:
1. √Äs `2022-07-27T17:06:00.238117` n√≥s recebemos 1 falha.
2. Logo em seguida, vemos que nosso client tenta novamente `2022-07-27T17:06:03.303864`, mais 1 falha pois ainda estamos dentro do intervalo de falha (00-05s).
3. Finalmente a nossa 3¬™ tentativa √© executada com sucesso `2022-07-27T17:06:06.316072`
4. Nossa execu√ß√£o continua como se n√£o houvesse nenhuma falha. Afinal, a chamada foi feita com sucesso com o uso de retries.

Bacana n√©? Mas como tudo em engenharia de software, a solu√ß√£o acima tem alguns problemas.

## Problemas com retries

Imagine a seguinte situa√ß√£o.

1. √â o in√≠cio de uma *black friday* e todo mundo decide acessar o seu servi√ßo ao mesmo tempo.
2. Ao tentar processar esse fluxo de requisi√ß√µes ao mesmo tempo o nosso pobre servidor n√£o aguenta e manda um erro tempor√°rio.
3. Recebendo o erro tempor√°rio, todos os clientes decidem tentar a requisi√ß√£o novamente.
4. As requisi√ß√µes com retry chegam todas de uma vez.
5. O nosso servidor continua sobrecarregado e continua falhando as requisi√ß√µes.
6. O usu√°rio fica infeliz porque perdeu aquela promo√ß√£o bacana...

Ser√° que d√° pra fazer melhor?

## Retries com *backoff and jitter*

[Nesse post de Mar√ßo de 2015](https://aws.amazon.com/blogs/architecture/exponential-backoff-and-jitter/) Marc Brooker, Senior Principal Engineer no AWS,  introduz a id√©ia de solucionar esse problema utilizando um algoritmo de retry com *backoff* e jitter. Como funciona essa ideia?

1. Ao inv√©s de executar a requisi√ß√£o de retry imediatamente ap√≥s a falha o cliente espera um pouco
2. Esse tempo de espera √© determinado seguindo uma f√≥rmula que leva em conta um pouco de aleatoriedade( essa √© parte chamada jitter) e uma parte exponencial relacionada ao n√∫mero de falhas (essa √© a parte de *backoff*).

### Por qu√™ *backoff*?

A ideia do *backoff* √© n√£o tentar a requisi√ß√£o imediatamente. Ao inv√©s disso, para casa falha, n√≥s dobramos o tempo de espera. Por exemplo, na primeira falha esperamos 2s pra tentar de novo, na segunda falha n√≥s esperamos 4s, na terceira falha 8s e assim sucessivamente. Essa estrat√©gia faz com que os retries n√£o continuem colocando press√£o em um servidor j√° sobrecarregado.

### Por qu√™ *Jitter*?

Por√©m o *backoff* n√£o resolve o problema completamente, imagine que todos os clientes falham ao mesmo tempo. Isso significa que em 2s o nosso servidor vai levar uma sobrecarga de requisi√ß√µes e em 4s de novo...
O *Jitter* adiciona um pouco de aleatoriedade pra melhorar essa estrat√©gia. Enquanto alguns clientes podem retentar em 2s, outros v√£o faz√™-los em 2.1s ou 2.2s. Essa pequena vari√¢ncia ajudar a espalhar o n√∫mero de requisi√ß√µes ao longo do tempo diminuindo a carga sobre a nossa aplica√ß√£o.

Se voc√™ quer mais, o [zanfranceschi tem uma thread bacana falando mais sobre backoff e jitter](https://dev.to/zanfranceschi/conceito-backoff-e-jitter-3a30).

## Mais problemas com retries

Voltando ao nosso problema da *black friday* imagine a segunda situa√ß√£o.

1. N√≥s adicionamos o algoritmo de backoff e jitter ao nossos retries.
2. Mesmo assim, a carga √© t√£o alta que o nosso servidor n√£o segue aguentar as requisi√ß√µes que continuam chegando em 2.1, 2.2 ou 2.3s depois.
3. Com a adi√ß√£o dos retries ao tr√°fego normal, toda vez que o servidor t√° pra se recuperar ele cai novamente.

A pergunta que fica √©, se a carga est√° t√£o alta e um retry sempre vai falhar, por que dever√≠amos continuar enviando retries?

## Retries com algoritmo de *token bucket*

Pra solucionar o problema acima n√≥s vamos utilizar um novo algoritmo de retries que √© a estrat√©gia de utilizar um  algoritmo de token bucket. Como funciona essa ideia?

1. Para cada request feita com sucesso n√≥s adicionamos uma percentagem de bucket em uma vari√°vel. Por exemplo: 0.1 token para cada sucesso.
2. Para cada falha n√≥s removes 1 token da vari√°vel.
3. N√≥s s√≥ podemos realizar chamadas enquanto o token for acima de 0.

Por exemplo:

1. Suponha que nosso token bucket come√ßa com um valor de 3.
2. Quando a primeira request falhar, n√≥s subtra√≠mos 1 do bucket. Agora o bucket tem o valor de 2.
3. Como o bucket tem o valor acima de 0, n√≥s executamos uma nova request.
4. Recebemos uma nova falha e o bucket cai para 1.
5. Tentamos novamente, acontece uma nova falha o bucket cai pra 0.
6. Os retries param totalmente.
7. Novas requisi√ß√µes podem ser feitas mas note que os retries n√£o vai ser feitos j√° que o bucket atingiu 0.
8. Quando as requisi√ß√µes come√ßarem a ter sucesso, elas come√ßam a adicionar 0.1 bucket para cada sucesso.
9. Depois de 10 requisi√ß√ïes com sucesso n√≥s temos um bucket de valor 1 e agora caso haja uma falha n√≥s teremos direito a 1 retry.


Esse algoritmo √© √≥timo para impedir o problema de retry storm que ocorre quando um servi√ßo A chama um servi√ßo B que chama um servi√ßo C. Um erro em cascata pode dar trigger em um retry de A que pode tentar at√© 3x, B pode tentar tamb√©m 3x o que faz com que C possa receber at√© 9x o tr√°fego üò± .

Esse algoritmo pode ser utilizado ao lidar com a AWS SDK utilizando a estrat√©gia de [TokenBucketRetryCondition](https://sdk.amazonaws.com/java/api/latest/software/amazon/awssdk/core/retry/conditions/TokenBucketRetryCondition.html).


## Ainda mais problemas com retries

Infelizmente, mesmo o token bucket n√£o soluciona todos os nossos problemas. Por exemplo:
1. O que acontece quando executamos o retry mas o servidor ainda est√° processando a 1¬™ requisi√ß√£o?
2. Se falharmos 2x, vale √† pena continuar consumindo recursos e tentar uma 3¬™ vez?
3. Toda vez que fazemos retry, ainda estamos segurando a resposta ao usu√°rio, o que faz com que a lat√™ncia da requisi√ß√£o aumente e logo introduz mais erros de timeout.
4. Devemos fazer retry no n√≠vel mais baixo, por exemplo em uma chamada de API, ou no n√≠vel mais alto apenas para evitar *retry storms*?

Esse artigo n√£o vai responde todas essas perguntas mas algumas delas podemos explorar em novos artigos no futuro.

## Conclus√£o

Nesse artigo n√≥s discutimos:
1. Erros tempor√°rios e porqu√™ eles acontecem.
2. Diferentes estrat√©gias de retry como linear, backoff and jitter e Token Bucket.
3. Diversos problemas ao se utilizar retries.

Como tudo em software, qual estrat√©gia voc√™ vai utilizar depende. As vezes, n√£o fazer o retry pode ser a melhor estrat√©gia. [Um post recente do Marc Brooker no blog pessoal dele](https://brooker.co.za/blog/2022/02/28/retries.html) discute em detalhes os trade-offs de cada estrat√©gia de retry e compara elas com circuit breakers, por exemplo.

Espero que tenham curtido o post, e se voc√™ curtiu, compartilha e d√° o like!


## Refer√™ncias
1. [Fixing retries with token buckets and circuit breakers](https://brooker.co.za/blog/2022/02/28/retries.html)
2. [Timeouts, retries, and backoff with jitter](https://aws.amazon.com/builders-library/timeouts-retries-and-backoff-with-jitter/?did=ba_card&trk=ba_card)
3. [Exponencial Backoff and Jitter](https://aws.amazon.com/blogs/architecture/exponential-backoff-and-jitter/)
4. [Summary of retry strategies](https://www.tomwalton.blog/posts/summary-of-retry-strategies/)